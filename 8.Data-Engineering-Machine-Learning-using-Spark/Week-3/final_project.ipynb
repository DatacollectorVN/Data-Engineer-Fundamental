{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28694fcb",
   "metadata": {},
   "source": [
    "![Skills Network Logo](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/IDSNlogo.png)\n",
    "\n",
    "# ETL and Machine Learning - Final Project\n",
    "\n",
    "**Estimated time needed:** 120 minutes\n",
    "\n",
    "In this lab you’ll create your own Apache Spark application as end to end use case from data acquisition, transformation, model training and deployment.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    "1.  Pull-in data from the HMP dataset\n",
    "2.  Create a Spark data frame from the raw data\n",
    "3.  Store this to parquet (in Cloud Object Store)\n",
    "4.  Read it again (from Cloud Object Store)\n",
    "5.  Deploy this model to Train a ML-Model on that data set\n",
    "6.  Watson Machine Learning\n",
    "\n",
    "But don’t be scared, we provide you with a set of sample notebooks you can modify and hook together. The library where you can draw the notebooks from is called CLAIMED – the Component Library for AI, Machine Learning, ETL and Data Science and is an open source library available on the [CLAIMED GitHub repo](https://github.com/IBM/claimed/tree/master/component-library).\n",
    "\n",
    "You’ll use [Elyra – a JupyterLab extension](https://elyra.readthedocs.io/en/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0231ENSkillsNetwork26766988-2021-01-01) for editing the notebooks and if you like you can use the [pipeline editor](https://elyra.readthedocs.io/en/latest/getting_started/overview.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0231ENSkillsNetwork26766988-2021-01-01#ai-pipelines-visual-editor) of Elyra to visually join them into a data pipeline.\n",
    "\n",
    "Elyra is the foundation of the [IBM Watson Studio Orchestration Flow](https://medium.com/ibm-data-ai/automating-the-ai-lifecycle-with-ibm-watson-studio-orchestration-flow-4450f1d725d6?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0231ENSkillsNetwork26766988-2021-01-01) tool which can be used in the cloud. Feel free to give it a shot as well to experience how a business user would do the job.\n",
    "\n",
    "We’ll use the HMP dataset you’re already familiar with from the ETL lab. The dataset is publically available [here](https://github.com/wchill/HMP_Dataset).\n",
    "\n",
    "Note that in the lab we have concentrated on the Extract (pulling data from the github repository) and Transform (parsing the raw files into a Apache Spark data frame) parts and didn’t load it to anywhere.\n",
    "\n",
    "In previous generation BigData systems, HDFS was the core data store. Nowadays, S3 compatible Cloud Object Store (COS) is the de-facto standard across clouds and also starts to get traction in local data centers (Ceph, Minio).\n",
    "\n",
    "So let’s have a quick look at the tooling:\n",
    "\n",
    "![Architecture Diagram of the component – interactions](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp1.png)\n",
    "\n",
    "Figure 1: Architecture Diagram of the component – interactions\n",
    "\n",
    "![Elyra and Jupyter Lab](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp2.png)\n",
    "\n",
    "Figure 2: This is a screenshot of Elyra and JupyterLab. You can see the pipeline editor canvas (1) which allows you to design data flow pipelines out of (Apache Spark based) Jupyter notebooks, Python and R scripts. You open a pipeline by clicking on the file (2) or create a new one (3). You can collaborate with others using the built-in git (4) functionality. Elyra automatically creates a Table of Contents of your notebooks (5) and useful code snippets are right at your fingertips using the code snipped repository (6).\n",
    "\n",
    "So let’s get started with the lab!\n",
    "\n",
    "### Note - Screenshots\n",
    "\n",
    "Throughout this lab you will be prompted to take screenshots and save them on your own device. These screenshots will be needed to be uploaded for peer review in the next section of the course. You can use various free screengrabbing tools to do this or use your operating system's shortcut keys to do this (for example Alt+PrintScreen in Windows).\n",
    "\n",
    "## Exercise 1 : Import the CLAIMED library to JupyterLab\n",
    "\n",
    "1.  This section uses the JupyterLab interface of Skills Network Labs which you are already currently in.\n",
    "\n",
    "2.  Please double check that you are reading this readme while in that environment.\n",
    "\n",
    "![jupyterlab button](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp3.png)\n",
    "\n",
    "3.  In a separate browser tab, please open the CLAIMED component library: [GitHub Link](https://github.com/IBM/claimed)\n",
    "4.  Don’t hesitate to give us a star (1) :), then please click on fork (2)\n",
    "\n",
    "(This way you will be able work on your own copy)\n",
    "\n",
    "![Elyra Github repo](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp4.png)\n",
    "\n",
    "5.  Please note down the new URL of the library, something like: `https://github.com/<your_git_user_id>/claimed`\n",
    "6.  In JupyterLab (which contains the Elyra extensions pre-installed) please click on Git (1), then “Clone”. Then please paste the URL above (2) and then click on “CLONE”. The screenshot below illustrates this step.\n",
    "\n",
    "![Clone component library to Jupyterlab](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp5.png)\n",
    "\n",
    "7.  After a while you should be able to see the following message bottom right:\n",
    "\n",
    "![sucess message](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp6.png)\n",
    "\n",
    "8.  Congratulations, you’ve successfully imported the component library.\n",
    "\n",
    "## Task 1 - Import the component library\n",
    "\n",
    "Take a screenshot of the success message along with the rest of the screen (full screenshot). Name the screenshot as `1-import-library.jpg`. (images can be saved with either .jpg or .png extension)\n",
    "This message is only available for a short period of time. In case you miss it just open the contents of the claimed/component-library folder in the file explorer and then take the screenshot of your ENTIRE screen.\n",
    "\n",
    "## Exercise 2 : Explore the CLAIMED component library\n",
    "\n",
    "Now it’s time to familiarize yourself a bit with some components (Jupyter Notebooks) in the component library.\n",
    "\n",
    "1.  In JuypterLab, please go to the file explorer (1), double-click on folder \"claimed\", then on  “component-library” (2)\n",
    "\n",
    "![jupyerlab exploration](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp7.png)\n",
    "\n",
    "2.  Please open the folder called \"transform\" (1) and open the notebook called spark-csv-to-parquet (2). The prefix \"spark\" indicates that the notebook is using Apache Spark to perform its task. From the name you can make out that this component transforms a file from \"csv\" to \"parquet\" format. Each notebook starts with a title and description of what it is supposed to do, followed by commands to install library dependencies. Then, a set of parameters this notebook accepts is provided (3), followed by an actual implementation of pulling those parameters from the environment (4). Finally, the actual task is implemented in source code (5).\n",
    "\n",
    "![jupyerlab exploration](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp8.png)\n",
    "\n",
    "3.  Please explore the other components in the library and have a look how they are implemented. They serve as cookie cutter to an abundance of daily data science tasks and hopefully you can learn from them.\n",
    "\n",
    "## Task 2 - Explore component library transformations\n",
    "\n",
    "For this task, please run the spark-parquet-to-csv notebook. Even though we are going to be using the spark-csv-to-parquet notebook later, the goal of this task is to familiarize the learners with the different data science functions included in the component library. Please note that this notebook will fail in the penultimate cell with *AnalysisException: 'Path does not exist: file:/resources/claimed/data/data.csv;'* - this is because we haven't created any data yet.\n",
    "\n",
    "Take a entire screen screenshot of the `spark-parquet-to-csv.ipynb` notebook with the cells executed. Name the screenshot as `2-parquet-to-csv.jpg`. (images can be saved with either .jpg or .png extension)\n",
    "\n",
    "## Exercise 3 : Create the Pipeline\n",
    "\n",
    "Now it’s time to create the data processing pipeline.\n",
    "Everything you need is available from within the CLAIMED component library, so please make heavy usage of the code provided. It is completely up to you which path you want to follow:\n",
    "\n",
    "1.  Implement the complete pipeline as a large, single jupyter notebook (works, but not recommended in production)\n",
    "2.  Break it up into smaller chunks\n",
    "3.  Use the components (jupyter notebooks) from CLAIMED 1:1 and just configure them using variables\n",
    "4.  Use the components (jupyter notebooks) from CLAIMED 1:1 and just configure them by dragging and dropping them to the pipeline editor of Elyra and set the environment variables using the user interface\n",
    "\n",
    "### 3A : Getting Started\n",
    "\n",
    "1.  Using the file explorer (1), enter directory \"claimed\".\n",
    "\n",
    "![creating a pipeline](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp9.png)\n",
    "\n",
    "2.  Let’s start with pulling the the HMP data set. In the component library under folder “input” (so in claimed/component-library/input) you’ll find a notebook called `“input-hmp.ipynb”`. Just open it by double-clicking and execute each cell, one by one until you get the result file.\n",
    "\n",
    "Note: A cell which creates a lot of output can be switched into scrolling mode. Just right-click into the output canvas (1) and click on “Enable Scrolling for Outputs” (2)\n",
    "\n",
    "![creating a pipeline](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp10.png)\n",
    "\n",
    "3.  After quite some time you should see a folder called “data.csv” in the “data” directory you’ve previously created. Please note that Apache Spark always creates folders containing individual files, one per partition. This is not a problem because Spark doesn’t distinguish between files and folders and threats folders as they where files. The only way to get a file is to repartition the data frame to one and extract/rename the file inside the folder `df = df.repartition(1)`.\n",
    "\n",
    "4.  Now it’s time to convert this CSV file to a parquet file, please use the spark-csv-to-parquet notebook in the transform category you’re already familiar with.\n",
    "\n",
    "5.  You might have noticed that the writing speed of parquet is far higher than the one of CSV files, this is already a glimpse of the effectiveness of the parquet format.\n",
    "\n",
    "## Task 3 - Convert CSV to Parquet\n",
    "\n",
    "Take a screenshot of the parquet file that is created at the end of this step. Name the screenshot as `3-converted-parquet.jpg`. (images can be saved with either .jpg or .png extension). You will find it in the *claimed/data* folder, parallel to the *component-library* folder. Just take a screenshot of your entire screen when you show the resulting file (it is actually a folder) in the file explorer.\n",
    "\n",
    "### 3B : Train the Machine Learning Model\n",
    "\n",
    "1.  Now it’s time to train a machine learning model given the data. Just open the `spark-train-lr.ipynb` notebook in the “train” category\n",
    "2.  Please use the default values and train a linear regression model using the data\n",
    "3.  After all cells have been executed, you’ll have a file called `model.xml` in the “data” folder\n",
    "\n",
    "PMML stand for “Predictive Model Markup Language” and is an interchange format for machine learning models. We’ll use this file to deploy to the IBM Watson Machine Learning Service in the next section.\n",
    "\n",
    "## Task 4 - Perform Model Training\n",
    "\n",
    "Please note the accuracy you get after training the model with the default parameters somewhere. You can round the output to 2 decimal places. You will be asked to submit this result later in text format.\n",
    "\n",
    "## Task 5 - Complete the Model Training\n",
    "\n",
    "Take a screenshot of the folder explorer window showing the `pmml` file. Name the screenshot as `5-pmml.jpg`. (images can be saved with either .jpg or .png extension)\n",
    "\n",
    "## Exercise 4 : Deploy the Model\n",
    "\n",
    "Now we deploy the model we’ve trained with Apache Spark to the IBM Watson Machine Learning (WML) Service. WML is a scalable, scale-to-zero cloud service with supports training and serving of machine learning and deep learning models, providing a HTTP(S) endpoint for seamless consumption from 3rd party applications. Advantages of such a service include:\n",
    "\n",
    "• Costs only incur if the service is actively used (scale-to-zero)\\\n",
    "• Response time is constant, independent of the number of parallel requests as auto-scaling is used\\\n",
    "• Operational costs are zero since the cloud provider is responsible for operation\n",
    "\n",
    "To use the service an `API_KEY` and `DEPLOYMENT_SPACE_GUID` is needed. The API_KEY provides access to all IBM Cloud services whereas the `DEPLOYMENT_SPACE_GUID` is used to publish the model to the Watson Machine Learning service. Therefore, in the following, an IBM Cloud account needs to be created and activated (1), an Instance of IBM Cloud Pak for Data needs to be started (2), within that, a Deployment Space needs to be created (3) and a Watson Machine Learning Service needs to be created and associated to the Deployment Space. At the end of this exercise, you have to note down the `API_KEY` and `DEPLOYMENT_SPACE_GUID`.\n",
    "\n",
    "![deploy model](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp11.png)\n",
    "\n",
    "### 4A: Create and Obtain API keys\n",
    "\n",
    "To obtain `API_KEY` and `DEPLOYMENT_SPACE_GUID` credentials, please follow these steps:\n",
    "\n",
    "1.  Open [the IBM Cloud Registration Page](https://dataplatform.cloud.ibm.com/registration/stepone?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0231ENSkillsNetwork26766988-2021-01-01) in your web browser.\n",
    "2.  Select “Dallas” (1) as IBM Cloud location.\n",
    "\n",
    "![deploy model](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp12.png)\n",
    "\n",
    "3.  If you already have an IBM Cloud account, complete the activation steps, otherwise please create a new account\n",
    "\n",
    "Click and open this [link](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-CC0100EN-SkillsNetwork/labs/IBMCloud_accountCreation/CreateIBMCloudAccount.md.html) and follow the instructions, to create an IBM Cloud account.\n",
    "Once completed, open the[IBM Cloud login page](https://dataplatform.cloud.ibm.com) and select “Dallas” as cloud region.\n",
    "\n",
    "![deploy model](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp13.png)\n",
    "\n",
    "4.  Click “Deployments” (1).\n",
    "\n",
    "![deploy model](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp14.png)\n",
    "\n",
    "5.  Create a new deployment space.\n",
    "6.  Enter “think” as name (1) and click “Create” (2).\n",
    "\n",
    "![deploy model](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp15.png)\n",
    "\n",
    "7.  Wait for the deployment space to be created.\n",
    "8.  Click “View new space” (1).\n",
    "\n",
    "![deploy model](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp16.png)\n",
    "\n",
    "9.  Select the “Manage” tab and locate the “Machine learning service” section.\n",
    "\n",
    "![deploy model](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp17.png)\n",
    "\n",
    "10. Click “Associate instance +” (1) to add Watson Machine Learning to the space.\n",
    "\n",
    "![deploy model](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp18.png)\n",
    "\n",
    "11. Select the free “Lite” (1) plan and click “Create” (2) if no existing Watson Machine Learning service was found.\n",
    "\n",
    "![deploy model](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp19.png)\n",
    "\n",
    "12. Select the Machine Learning service instance you’ve just created. Note that the name might be slightly different in your environment.\n",
    "\n",
    "![deploy model](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp20.png)\n",
    "\n",
    "13. Make sure you’re still on the “Manage” (1) tab and then click the copy button for the `DEPLOYMENT_SPACE_GUID` (2). (Note that highlighting and copying the ID doesn’t work because only a portion is displayed.)\n",
    "\n",
    "![deploy model](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp21.png)\n",
    "\n",
    "14. Paste the copied space ID into a file for later use.\n",
    "\n",
    "    (Create an IBM Cloud API key)\n",
    "\n",
    "15. Open the [Cloud API key page](https://cloud.ibm.com/iam/apikeys?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0231ENSkillsNetwork26766988-2021-01-01).\n",
    "\n",
    "16. Click “Create an IBM Cloud API key”.\n",
    "\n",
    "![deploy model](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp22.png)\n",
    "\n",
    "17. Name the API key “think” (1) and click “Create” (2).\n",
    "\n",
    "![deploy model](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp23.png)\n",
    "\n",
    "18. Copy the API_KEY to your clipboard and paste it to the text editor which already contains the `DEPLOYMENT_SPACE_GUID`.\n",
    "\n",
    "Congratulations - you’ve obtained all information required to customize the pipeline for deployment.\n",
    "\n",
    "### 4B: Deploy The Trained Model\n",
    "\n",
    "1.  Re-open the JupyterLab/Elyra web browser tab in skillsnetwork.\n",
    "2.  Open the “deploy/deploy_wml_pmml” component and fill in the `API_KEY` and `DEPLOYMENT_SPACE_GUID`\n",
    "3.  Run all cells\n",
    "4.  Open the [ML runtime spaces page](https://dataplatform.cloud.ibm.com/ml-runtime/spaces?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0231ENSkillsNetwork26766988-2021-01-01)\n",
    "5.  Select your space\n",
    "6.  Identify your model (1) and click on the “rocket” symbol (2) to deploy your model\n",
    "\n",
    "![deploy model](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/labs/images/fp25.png)\n",
    "\n",
    "7.  Select “Online”, give it a name and click on “Create”\n",
    "\n",
    "8.  Click on “Deployments”, then identify your deployment and click on it.\n",
    "\n",
    "9.  Click on “Python” to obtain sample code on how to call your model\n",
    "\n",
    "10. Copy the complete python sample code into a jupyter notebook\n",
    "\n",
    "11. Paste the IBM Cloud `API_KEY` from the last section to the appropriate location in the code\n",
    "\n",
    "12. Replace the line starting with with the following line:\n",
    "\n",
    "    ```Python\n",
    "    payload_scoring = {\"input_data\": [{\"fields\": [\"x\", \"y\", \"z\", ], \"values\": [[1,2,3]]}]}\n",
    "    ```\n",
    "\n",
    "13. Execute the code. You should see the result of the prediction\n",
    "\n",
    "## Task 6 - Deploy the model to Watson Machine Learning\n",
    "\n",
    "After you ran all steps in the \"deploy/deploy_wml_pmml\" notebook, please go to the ML runtime spaces page as mentioned in step 4 above. Take a screenshot of the Watson Studio page showing your model deployment. Name the screenshot as `6-deploy_wml_pmml.jpg`. (images can be saved with either .jpg or .png extension)\n",
    "\n",
    "## Task 7 - Perform Model Inference\n",
    "\n",
    "Please upload a screenshot showing the model output for the input provided in step 12 above. In the next cell of the notebook also come up with an invalid input that results in an error message being generated. The goal of this exercise is to see potential ways our model deployment API can be fed with user errors/invalid inputs. This will help correct such errors in the future.\n",
    "You can title the screenshot `7-model-inference.jpg`.\n",
    "\n",
    "## Task 8 - HyperParameter Tuning\n",
    "\n",
    "For this task, we are going to perform hyperparameter tuning. We will be reverting back to the notebook used in \"3B : Train the Machine Learning Model\" section.\n",
    "\n",
    "Hyperparameters are training parameters that can be controlled by us during the training process. Examples include learning rate, number of iterations, steps per iteration etc. Specifically in the notebook used in 3B, we have 3 hyper parameters and their default values - namely `maxIter=10`, `regParam=0.3` and `elasticNetParam=0.8`. Now, for this exercise, do the following:\n",
    "\n",
    "*   Iterate over the hyperparameters maxIter in \\[10, 100, 1000], regParam in \\[0.01, 0.5, 2.0] and elasticNetParam in \\[0.0, 0.5, 1.0].\n",
    "*   Print the accuracy for each combination of hyperparameters in a human readable format.\n",
    "*   Report the combination of hyperparameters that yielded the highest accuracy\n",
    "\n",
    "Take a screenshot of the above 3 notebook cells and title it `8-hyper-parameter-tuning.jpg`.\n",
    "\n",
    "## Task 9 - Resample data splits\n",
    "\n",
    "For this task, you will use the best hyperparameter combinations used above but re-train the model with different train and test splits.\n",
    "Perform training with the best hyperparameters from the above task with the following splits:\n",
    "\n",
    "*   70:30 train:test split\n",
    "*   90:10 train:test split\n",
    "\n",
    "Note: for both the splits above, use a `random seed` of 1.\n",
    "\n",
    "For both the training above, report the accuracy on the test split and save a screenshot called `9-resample-data-splits.jpg`.\n",
    "\n",
    "## Task 10 - RandomForest classification\n",
    "\n",
    "In this task, you will build an end to end pipeline that reads in data in parquet format, converts it to CSV and loads it into a dataframe, trains a model and perform hyperparameter tuning. For this submission, you may use code and snippets from all the resources mentioned above including the component library. Create a notebook that does the following:\n",
    "\n",
    "*   Read in the `parquet` file you created as part of Task 3.\n",
    "\n",
    "*   Convert the `parquet` file to `CSV` format.\n",
    "\n",
    "*   Load the CSV file into a dataframe\n",
    "\n",
    "*   Create a 80-20 training and test split with `seed=1`.\n",
    "\n",
    "*   Train a Random Forest model with different hyperparameters listed below and report the best performing hyperparameter combinations.\n",
    "\n",
    "    Hyper parameters:\n",
    "\n",
    "    ```\n",
    "      - number of trees : {10, 20}\n",
    "      - maximum depth : {5, 7} \n",
    "      - use random seed = 1 wherever needed\n",
    "    ```\n",
    "\n",
    "*   Use the accuracy metric when evaluating the model with different hyperparameters\n",
    "\n",
    "Title your notebook `randomforest.ipynb` and export it as HTML. This should create a file called `randomforest.html` which you will later submit.\n",
    "\n",
    "> Note :If you are unable to upload the html file , you can upload the pdf file which is `randomforest.pdf`\n",
    "\n",
    "## Author(s)\n",
    "\n",
    "[Romeo Kienzler](https://www.coursera.org/instructor/romeo-kienzler?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0231ENSkillsNetwork26766988-2021-01-01)\n",
    "\n",
    "[Karthik Muthuraman](https://www.linkedin.com/in/karthik-muthuraman/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0231ENSkillsNetwork26766988-2021-01-01)\n",
    "\n",
    "## Changelog\n",
    "\n",
    "| Date       | Version | Changed by         | Change Description       |\n",
    "| ---------- | ------- | ------------------ | ------------------------ |\n",
    "| 2021-05-12 | 0.1     | Romeo Kienzler     | Initial version created  |\n",
    "| 2021-06-22 | 1.0     | Karthik Muthuraman | First editorial pass     |\n",
    "| 2021-05-12 | 1.1     | Romeo Kienzler     | Adjusted for API changes |\n",
    "| 2021-08-09 | 1.2     | Romeo Kienzler     | Move to CLAIMED git url  |\n",
    "| 2021-08-10 | 1.3     | Romeo Kienzler     | Migrate to notebook      |\n",
    "| 2021-08-10 | 1.4     | Romeo Kienzler     | Bugfixes                 |\n",
    "| 2021-08-11 | 1.5     | Karthik Muthuraman | Bugfixes and typos       |\n",
    "| 2021-08-30 | 1.6     | Karthik Muthuraman | Post Beta Feedback       |\n",
    "\n",
    "## <h3 align=\"center\"> © IBM Corporation 2021. All rights reserved. <h3/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f51e21-5828-441f-907d-0cabf81b4059",
   "metadata": {},
   "source": [
    "### Task-3A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "798d6df1-e4d4-495a-97c3-a9cb75b96ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMP_Dataset  claimed  data  etl_lab.ipynb  final_project.ipynb\tinstall.log\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccd02176-9f3c-4699-ae7c-bf8a4c6fad67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[22;0t\u001b]0;IPython: labs/BD0231EN\u000737\n",
      "Starting installation...\n",
      "Please <<RESTART YOUR KERNEL>> (Kernel->Restart Kernel and Clear All Outputs)\n",
      "WARNING:root:Parameter: data_dir=\"./claimed/data/\"\n",
      "22/02/06 13:17:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/06 13:17:22 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/02/06 13:17:22 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "[Stage 2:>                                                        (0 + 16) / 27]22/02/06 13:17:55 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (992,660,672 bytes) of heap memory\n",
      "Scaling row group sizes to 92.45% for 8 writers\n",
      "22/02/06 13:17:55 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (992,660,672 bytes) of heap memory\n",
      "Scaling row group sizes to 82.18% for 9 writers\n",
      "22/02/06 13:17:55 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (992,660,672 bytes) of heap memory\n",
      "Scaling row group sizes to 73.96% for 10 writers\n",
      "22/02/06 13:17:55 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (992,660,672 bytes) of heap memory\n",
      "Scaling row group sizes to 67.24% for 11 writers\n",
      "22/02/06 13:17:55 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (992,660,672 bytes) of heap memory\n",
      "Scaling row group sizes to 61.63% for 12 writers\n",
      "22/02/06 13:17:55 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (992,660,672 bytes) of heap memory\n",
      "Scaling row group sizes to 56.89% for 13 writers\n",
      "22/02/06 13:17:55 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (992,660,672 bytes) of heap memory\n",
      "Scaling row group sizes to 52.83% for 14 writers\n",
      "22/02/06 13:17:55 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (992,660,672 bytes) of heap memory\n",
      "Scaling row group sizes to 49.31% for 15 writers\n",
      "22/02/06 13:17:55 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (992,660,672 bytes) of heap memory\n",
      "Scaling row group sizes to 46.22% for 16 writers\n",
      "22/02/06 13:18:01 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (992,660,672 bytes) of heap memory\n",
      "Scaling row group sizes to 49.31% for 15 writers\n",
      "22/02/06 13:18:01 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (992,660,672 bytes) of heap memory\n",
      "Scaling row group sizes to 52.83% for 14 writers\n",
      "22/02/06 13:18:01 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (992,660,672 bytes) of heap memory\n",
      "Scaling row group sizes to 56.89% for 13 writers\n",
      "22/02/06 13:18:01 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (992,660,672 bytes) of heap memory\n",
      "Scaling row group sizes to 61.63% for 12 writers\n",
      "22/02/06 13:18:01 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (992,660,672 bytes) of heap memory\n",
      "Scaling row group sizes to 67.24% for 11 writers\n",
      "22/02/06 13:18:01 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (992,660,672 bytes) of heap memory\n",
      "Scaling row group sizes to 73.96% for 10 writers\n",
      "22/02/06 13:18:01 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (992,660,672 bytes) of heap memory\n",
      "Scaling row group sizes to 82.18% for 9 writers\n",
      "22/02/06 13:18:01 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (992,660,672 bytes) of heap memory\n",
      "Scaling row group sizes to 92.45% for 8 writers\n",
      "[Stage 2:===============================>                        (15 + 12) / 27]22/02/06 13:18:03 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (992,660,672 bytes) of heap memory\n",
      "Scaling row group sizes to 92.45% for 8 writers\n",
      "22/02/06 13:18:03 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (992,660,672 bytes) of heap memory\n",
      "Scaling row group sizes to 82.18% for 9 writers\n",
      "22/02/06 13:18:03 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (992,660,672 bytes) of heap memory\n",
      "Scaling row group sizes to 73.96% for 10 writers\n",
      "[Stage 2:=================================>                      (16 + 11) / 27]22/02/06 13:18:03 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (992,660,672 bytes) of heap memory\n",
      "Scaling row group sizes to 67.24% for 11 writers\n",
      "22/02/06 13:18:03 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (992,660,672 bytes) of heap memory\n",
      "Scaling row group sizes to 73.96% for 10 writers\n",
      "[Stage 2:===================================>                    (17 + 10) / 27]22/02/06 13:18:04 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (992,660,672 bytes) of heap memory\n",
      "Scaling row group sizes to 82.18% for 9 writers\n",
      "22/02/06 13:18:04 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (992,660,672 bytes) of heap memory\n",
      "Scaling row group sizes to 92.45% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "!ipython ./claimed/component-library/transform/spark-csv-to-parquet.ipynb data_dir=./claimed/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76043a5-6b37-4455-b529-ff7131bdb59e",
   "metadata": {},
   "source": [
    "### Task 4B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0aec1cc-8dbd-49bf-8115-b20b527b1179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring response\n",
      "{'predictions': [{'fields': ['prediction', 'probability(Walk)', 'probability(Getup_bed)', 'probability(Drink_glass)', 'probability(Pour_water)', 'probability(Climb_stairs)', 'probability(Eat_meat)', 'probability(Brush_teeth)', 'probability(Standup_chair)', 'probability(Sitdown_chair)', 'probability(Comb_hair)', 'probability(Descend_stairs)', 'probability(Use_telephone)', 'probability(Liedown_bed)', 'probability(Eat_soup)'], 'values': [[0.0, 0.20674808767118283, 0.10313120640491465, 0.09589091990996985, 0.09328331223165168, 0.09033681020700506, 0.06979015075948657, 0.06656082769592746, 0.05692667711572404, 0.05569722222837521, 0.05248776006136356, 0.03440358089242028, 0.03424954940864776, 0.025553850849017488, 0.014940044564313434]]}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# NOTE: you must manually set API_KEY below using information retrieved from your IBM Cloud account.\n",
    "API_KEY = \"5RM9U3kKUW8hrThqtZhkm0le6AAB3bdsR5nea2fsHzVz\"\n",
    "token_response = requests.post('https://iam.cloud.ibm.com/identity/token', data={\"apikey\": API_KEY, \"grant_type\": 'urn:ibm:params:oauth:grant-type:apikey'})\n",
    "mltoken = token_response.json()[\"access_token\"]\n",
    "\n",
    "header = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + mltoken}\n",
    "\n",
    "# NOTE: manually define and pass the array(s) of values to be scored in the next line\n",
    "#payload_scoring = {\"input_data\": [{\"fields\": [array_of_input_fields], \"values\": [array_of_values_to_be_scored, another_array_of_values_to_be_scored]}]}\n",
    "payload_scoring = {\"input_data\": [{\"fields\": [\"x\", \"y\", \"z\", ], \"values\": [[1,2,3]]}]}\n",
    "\n",
    "response_scoring = requests.post('https://us-south.ml.cloud.ibm.com/ml/v4/deployments/9069bbc8-4455-4c3b-ae04-f3b3a2132014/predictions?version=2022-02-06', json=payload_scoring, headers={'Authorization': 'Bearer ' + mltoken})\n",
    "print(\"Scoring response\")\n",
    "print(response_scoring.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d55629-f7f2-492f-a022-5810ce031994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
